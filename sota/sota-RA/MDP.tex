\newpage
\subsection{Markov Decision Processes}
% \GB{not intro on why MDP and no articulation with previous section}
In this section we describe the formalism of a Markov Decision Process (MDP).
While Game Theory models consider at least two players in a game looking to maximize their profit, an MDP represents a single agent interacting with a system and aiming at maximizing a reward. 

\subsubsection{Definitions}
% \GB{this paragraph is inconsistent with how you structured 2.5.1.1}
A Markov Decision Process~\cite{bellman1957} (MDP) is a mathematical model used to describe the interactions  an agent may have with a system.
The system is described by the set of state it can be in, and when in a particular state the agent will choose an action to perform.
Each action is associated to a potentially negative reward, considered as an incentive (or deterent) for the agent.
However, the outcome of each action is probabilistic thus incurring a certain amount of uncertainty about how the system will evolve.
Therefore, after choosing an action the system may transition towards one out of several states (or remain in the same state), according to a probability distribution.


Formally, an MDP is defined by a 4-tuple $<S,A,P(s,s',a),R(s,a,s')>$ where:
\begin{itemize}
    \item $S$ is a finite set of states
    \item $A$ is a finite set of actions
    \item $P(s,a,s') : S \times A \times S \longrightarrow [0,1]$ is a transition probability function
    \item $R(s,a,s') : S \times A \times S \longrightarrow \mathbb{R}$ is a reward function
\end{itemize}

The aim of an MDP is to determine an optimal policy, defining for each possible state which action will maximize the reward gained by the user while accounting for the future consequences of an immediate choice.

\subsubsection{MDP resolution}
% \GB{what about ``MDP resolution'' as a title?}
% \label{sec:optimalpolicy}
% Before determining the optimal policy for an MDP, we have to dive into the notion of reward of an MDP. Any action chosen in a specific state may reward the agent. However, the choice made now may also lead the agent to negative consequences that will not compensate for the immediate reward obtained. 
% This kind of drawback is mitigated with the notion of a long term reward, called utility. The utility of a state is the immediate reward gained when entering this state plus the rewards that can be obtained from reachable states.
% These future rewards are impacted with a discount factor, because the principle of uncertainty makes them less attractive than the immediate reward.
% With all these considerations, we can now determine the optimal policy for the defender.
The solution of an MDP consists in determining the action that will maximize the utility function for every possible state.

\subparagraph{The Bellman equation}
Bellman proposes in~\cite{bellman1957} an analysis of equations that will help determining the maximum expected reward in an MDP, expressed as non-linear equations. However he proves that they possess quasi-linear properties that allow him to solve these equations using methods only applicable to linear cases. 
The Bellman equation shown in \eqref{eq:bellmaneq} lays the groundwork for solving MDPs using dynamic programming.
This equation states that the optimal utility $V(s)$ expected from a state $s$ is the sum of the reward obtained when entering state $s$ and the expected discounted sum of probable utilities of the neighbours of $s$. The uncertainty of obtaining a reward in the future makes it less attractive than an immediate reward, $\gamma$ is the discount factor for the future reward.

\begin{equation}
V(s) = \max\limits_a \left \{\sum\limits_{s'}P(s,a,s')[R(s,a,s') + \gamma    V(s')  ] \right \}
\label{eq:bellmaneq}
\end{equation}


% \GB{below paragraph is a disgrace. tie the notation remark to the Equation (1) and mention earlier that utility is also known as value}
% \textbf{Vocabulary and notation clarifications}\\
% $R(s,a,s')$ is sometimes simplified to $R(s,a)$ in the literature.\\
% The term value is sometimes used interchangeably with utility.




The solution of an MDP is the optimal policy $\pi(s)$, which is a vector defining the optimal action for each possible state in the MDP.
There are three main algorithms to determine the optimal policy: Value Iteration~\cite{bellman1957}, Policy Iteration~\cite{policyiteration} and Q-Learning~\cite{qlearning}.
We detail those algorithms hereafter.
% There are several algorithms to determine the optimal policy, notably Value/Policy Iteration and Q-Learning.
% We will focus on Value Iteration because Q-Learning is applied to systems where the transition model is unknown, and Policy Iteration is very similar to Value Iteration.


\subparagraph{Value Iteration Algorithm}\textbf{\\}
% \GB{notation of reward is inconsistent between steps 2 and 4}
This algorithm~\cite{bellman1957} takes an iterative approach to determine the value of each state in the MDP with the Bellman equation.
Value Iteration works as follows:
\begin{enumerate}
    \item Initialize a value vector $V_0(s)=0,~\forall s\in S$
    \item Determine the utility of each state iteratively\\{$V_{k+1}(s) = \max\limits_a \left \{\sum\limits_{s'} P(s,a,s')[R(s,a,s') + \gamma V_k(s')] \right \}$}
    \item Repeat step 2 until $V_{k}(s)$ converges
    \item Compute the optimal policy $\pi(s)$\\$\pi(s) = \argmax\limits_a \left \{\sum\limits_{s'}P(s,a,s')[R(s,a,s') + \gamma V(s')] \right \}$
\end{enumerate}

% \FC{Don't forget misplacement of "The computational complexity etc.}
% \newpage
\subparagraph{Policy Iteration Algorithm}\textbf{\\}
Similarly to the previous algorithm, the Policy Iteration algorithm~\cite{policyiteration} works iteratively but directly determines the optimal policy instead of the value of the states.
Policy Iteration can be described as follows:
% \GB{line 4, you mention U(s). is it not V(s)?}
\begin{enumerate}
    \item Initialize a value vector and a policy vector $~\forall s\in S,\pi(s)=a\in A,~V(s)=0$
    \item Evaluate the policy
        \begin{algorithm}
            \Repeat{$\Delta < \Theta$ (a small positive number)}{
                $\Delta = 0$\\
                \ForEach{$s\in S$}{
                    $temp = V(s)$\\
                    \tcc{Compute utility}
                    $V(s) =\sum\limits_{s'} P(s,a,s')[R(s,a,s') + \gamma V(s')]$\\
                    $\Delta = max(\Delta,|temp-V(s)|)$
                }
            \tcc{Repeat until convergence of the utility}
            }
        \end{algorithm}\newpage
    \item Improve policy
    \begin{algorithm}
        stable-policy = True
        \ForEach{$s\in S$}{
            $temp = \pi(s)$\\
            \tcc{Compute new policy}
            $\pi(s) = \argmax\limits_a \left \{ P(s,a,s')[R(s,a,s') + \gamma \sum\limits_{s'}V(s')] \right \}$\\
            \tcc{Did the policy change ?}
            \uIf{$temp \neq \pi(s)$}{stable-policy=False}
        }
        \uIf{stable-policy}{exit}
        \tcc{If the policy changed recompute states values}
        \uElse{goto Step 2}
    \end{algorithm}
\end{enumerate}



The computational complexity of both algorithms is studied in~\cite{mdpcomplexity}.
Results show that MDPs can be solved in a polynomial time for both algorithm, but existing solutions are barely exploiting the structure specificities of an MDP. Simply put, MDPs are solved like any other optimization problem.

\subparagraph{Q-Learning}
The Q-Learning algorithm~\cite{qlearning} is used when the MDP does not provide an explicit definition of its model (\ie transitions and/or rewards definitions are unknown).
The agent only knows what state he is in, and upon choosing an action the agent will receive a reward for transitioning from state $s_t$ to $s_{t+1}$.
The Q function defines the \textit{quality} of action $a$ at state $s_t$.
Similarly to the Value Iteration algorithm, updating the Q function starts with an arbitrary initialization and will be gradually updated as time passes (See Equation~\eqref{eq:updatingq}).

% \GB{please comment Eq. (2)}
\begin{equation}\label{eq:updatingq}
Q'(s_{t},a_{t}) = (1-\alpha) \cdot \underbrace{Q(s_{t},a_{t})}_{\text{old value}} + \underbrace{\alpha}_{\text{learning rate}} \cdot  \overbrace{\bigg( \underbrace{r(s_t,s_{t+1})}_{\text{observed reward}} + \underbrace{\gamma \max_{a}Q(s_{t+1}, a)}_{\text{optimal discounted value}} \bigg) }^{\text{learned value}}
\end{equation}


% \FC{DON'T FORGET TO CHANGE FIGURE TO EQUATION, STILL BUGGY}

\subsubsection{MDP-based Resource Allocation}
We consider here the use of MDPs in the context of the resource allocation problem.
% We divide the following works into two categories, whether the MDP is used to process resource allocation requests or to directly allocate resources on an infrastructure.
Table~\ref{tab:MDP-summary} summarizes the surveyed works.

% \subparagraph{Allocation requests optimization}
% \textbf{\\}
% Wireless networks are subjects to several constraints because of the heterogeneous devices composing them and the varying quality of service due to external causes. Seamless transition between networking technologies are a great part of the user experience quality. In~\cite{navarro2008}, the problem of vertical handoff, \ie changing from one technology to another (\eg 5G to WiFi), is modeled to determine the optimal transitions that will provide the best service quality. 
% The MDP in this article uses \textit{time epochs} which are a discrete representation of the time.
% The state space of the MDP describes the different configurations in terms of available bandwidth, average delay and identification information for the current network.
% The actions available are simply whether to remain with the same networking technology or to switch for another. The reward function considers the quality of service and the cost of switching to another technology.
% Authors describes implementations issues related to the 802.21 standard and implement several extensions to their current model including QoS parameters and handling horizontal handoff (\ie switch from one base station to another).
% Extensive evaluations using real life use cases are performed and results are compared with a set of well known heuristics.

Using an MDP to allocate security services to cloud users is studied in~\cite{Liang2011}.
The authors start by describing the security services provided by the cloud, and the underlying RA problem.
They consider that the arrival of security services requests follow a Poisson law, and the system will either accept or reject the request. Formally speaking, the authors use a Semi MDP (SMDP) which is an extension of traditional MDP that include asynchronous events from outside the system and continuous time (in opposition to the discrete time approach of an MDP).
The states of the MDP represent the number of security services currently served to the users. In addition to requests arrival events, the departure of users (incurring a freeing of the resources) is also modeled as an event. The reward function substracts the cost of occupation of the cloud resources by the security services to the revenues generated by allocating resources to users. 
From this resource allocation model the blocking probability is derived, \ie which class of service is most likely to be rejecting requests from users. Extensive numerical evaluations are performed to determine optimal revenues under different request arrival rates.

RA problems become even more relevant in environments with scarce resources.
Allocating the right amount of resources in mobile Cloud networks will improve the user experience and failing to deliver will hurt the business model.
Such approach is considered in~\cite{Liang2012} where the authors use an MDP to handle resource transfer between mobile cloud domains. Each cloud domain composing the network can host a certain amount of services using its own resources. 
This work also uses the SMDP model because the request arrival formalism is well defined.
Arrival requests are handled by the receiving domain but when the domain's capacities are exceeded, service request are transferred to adjacent domains and determines which one is best suited to accept the request.
The state of a cloud domain is characterized by the number of services it is supporting and the type of request it receives.
The actions available are accepting, rejecting or transferring the request.
The reward system is based on the price paid by the user, the potential cost to transfer the service to an adjacent domain, but also the resources consumed to process the requests. 
This work proposes a more detailed resource consumption model in comparison to~\cite{Liang2011}.

% \subparagraph{Resource allocation optimization}
% \textbf{\\}
Using an MDP in resource allocation can become computationally intractable in case of an exponentially large state space, especially in large physical systems like data centers. In order to provide scalability to the resource allocation problem, a combination of queuing models and MDP is proposed in~\cite{Tesauro2006}.
Autonomic management in server clusters requires to minimize the number of human interactions required to configure and maintain the system.
However, providing such automated management requires to be able to choose the optimal configuration when needed.
This is an approach typically based on Q-Learning because the evolution of the system is not completely formalized.
The global aim is to allocate servers to trading applications that will maximize the SLA revenues, \ie the business value.
The authors state that doing a live training of the model leads to performance issues. Therefore, they prefer an offline training using typical queuing model. The MDP is trained offline using a previously collected dataset (states) and this training is completed with a queuing model that provides management decisions as the initial policy (actions).
The evaluation of the work is done on a realistic data-center prototype, hosting a trading application. 
Several scenarios are considered: open and closed-loops in the traffic as well as the delay incurred by allocating resources.

% Wang~\etal propose in~\cite{Wang2013} a resource allocation solution for CPU requests in cloud environments.
% They divide the problem into two sub-problems: configuring the voltage and frequency of each core to optimize request treatment and providing an optimal request dispatching solution that maximize the revenue generated while accounting for several parameters.
% Costs are estimated using the energy consumption of each core in the cluster and the average response time it takes to process a request.
% Authors model the service queuing problem as a Continuous Time MDP (CTMDP) for each cluster core, in opposition to the SMDP proposed earlier.
% They also use a Poisson law to model the arrival rate of requests but do not extend the representation to the actual events of incoming requests.
% The states of the MDP are defined by the queue length of the core, limited to a maximum queue size. The actions available are the frequency applied by the core to process requests.
% The reward function considered is a cost function that should be minimized.
% The energy consumption considers both the dynamic energy consumption due to processing requests as well the static power consumed by an idle core.
% The MDP solutions are then integrated into an optimization under constraints problem that will determine the optimal resource allocation to maximize the profits obtained from serving users while considering energy consumption.
% This problem is then solved using an algorithm maximizing the sum of linear fractional functions.


% Authors in~\cite{Bokani2015} consider a similar situation where the problem of HTTP-based streaming is performed in a vehicular environment.
% HTTP-based streaming uses small chunks of video stored on the server with different quality version to provide a seamless streaming experience thanks to adaptative algorithms. 
% These algorithms determine what is the best quality chunk that can be delivered to the user considering the current available bandwidth. Instead of using \textit{time epochs} to represent the buffering of the video and the deadline before video freezing, the length of buffered video time is used along the quality level of the current chunk to describe the states of the MDP. 
% Actions of the MDP represent the quality of the next chunk to be downloaded by the user.
% The reward function is based on the quality of the downloaded chunk, and penalties are applied if the video froze while playing. Additionally, a penalty for changing the quality level between two consecutive chunks is also applied.
% The MDP is then solved using the Value Iteration algorithm, but this model requires to know beforehand the bandwidth model. 
% These limitations can be overcome by using a Q-Learning function where the bandwidth map is unknown but instead the MDP will be trained.

\subsection{Summary}
% \GB{since it is a subsection of SoTA on MDP, this summary should only consider MDPs at first and only compare to GT at the end or in the main conclusion (2.6). please rewrite}
The resource allocation problem has been investigated using two different formalisms, namely the Game Theory formalism and Markov Decision Processes.
Game Theory accounts for the point of view of an attacker and a defender, allowing to extensively model the behavior of an attacker and his impact on the defended system.\\
The MDP ends up being a one user optimization method but the description of the reward function and the transitions between states may reflect external interactions, which could represent the impact of an attacker.
A strong advantage of MDPs over game theory models lies in the flexibility of the formalism to describe the available actions.

% \GB{Table~\ref{tab:MDP-summary} lacks the type of MDP (e.g., semi) and the reward}
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
Reference                           & States                      & Actions                         & Reward                            & Summary                                         \\ \hline
Liang~-~\cite{Liang2011}   & Number of security services & Acceptation/Refusal of requests & \multirow{3}{*}{Financial profit} & Security services allocation in the cloud       \\ \cline{1-3} \cline{5-5} 
Liang~-~\cite{Liang2012}   & Number of services deployed & Accept/Reject/Transfer requests &                                   & Service allocation in mobile cloud networks     \\ \cline{1-3} \cline{5-5} 
Tesauro~-~\cite{Tesauro2006} & Available servers           & Allocation decision             &                                   & Offline training of an MDP using queuing models \\ \hline
\end{tabular}%
}
\caption{Summary of RA solutions using MDP}
\label{tab:MDP-summary}
\end{table}

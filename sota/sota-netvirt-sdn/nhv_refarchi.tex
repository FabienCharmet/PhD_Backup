\subsection{Reference architecture of a network hypervisor}
\label{sec:reference_archi}
%\GB{a good start is the paper from Casado et al., published at PRESTO'10, where the idea of a network hypervisor is introduced. A quick introduction of network virtualization in general, and how SDN stands in this landscape, would be helpful. Please refer to the following website: \url{http://packetpushers.net/sdn-network-virtualization-hypervisors/}. I suggest to even go as far as to mention the state of the art of network virtualization: from VLANs and VPNs, to network slicing, to network hypervisors}
A reference architecture is a template for the architecture of a software solution, and defines a vocabulary to compare existing implementations. 
We consider here the design of a network hypervisor supporting a secure virtual network migration.
A network hypervisor should support the following operations:
\begin{itemize}
    \item Providing an abstraction of the physical infrastructure to tenants
    \item Providing an interface for tenants to interact with their virtual network
    \item Ensuring isolation between tenants to prevent undesired interactions 
    \item Enabling automated virtual network migration in case of attacks or failures
    \item Ensuring security of the hypervisor and the virtual networks
\end{itemize}

Casado~\etal propose a first formalization of a general purpose network hypervisor in~\cite{Netvirt_Definition-Casado2010}. They decouple the virtual network view from the physical infrastructure, and the network hypervisor enforces the mapping between the logical and physical planes.

The authors illustrate the processing of incoming traffic by simultaneously representing it in the logical plane and in the physical plane.
The logical plane performs a lookup to determine the next logical node to forward the traffic to and then the decision is transmitted to the physical plane. There, another lookup is performed to determine what is the physical equivalent of the logical forwarding decision.

The analysis of the different works presented in the previous section illustrates that this double lookup approach has not been considered to be a suitable solution for the virtual to physical mapping. The majority of existing hypervisors either slices the physical network, or maintains a mapping between physical and virtual elements. However, this mapping is only used to translate flow rules from the virtual flowspace to the physical one. Casado~\etal's  implementation of a network hypervisor prototype does not seem to include regular lookups but only maintain a mapping like all the other related solutions.

We propose the following reference architecture to highlight the important features that should be available for a secure network migration, and also to outline some shortcomings in existing solutions.

\input{figures/nvh-archi.tex}
  
Figure~\ref{fig:reference-archi-nh} presents the reference architecture of a network hypervisor supporting a secure virtual network migration.
On a top-down perspective, there are three different levels in the virtualization architecture.
The tenant level, where end-users can deploy their own applications, SDN controllers, and interact with their virtual network.
The hypervisor level, including all the components required to achieve network virtualization, present the tenant with an abstract view, and interact with the physical infrastructure.
The lowest level includes all the networking equipments, like SDN-enabled switches and routers.

\subsubsection{Control Plane Abstraction}
The Control Plane Abstraction component (CPAC) provides the tenants with an interface to interact with the rest of the virtualization layer.
This component is an essential function for network virtualization and is implemented in every hypervisor.
When a tenant requests a virtual network, he describes his requirements in terms of virtual nodes, links, minimal bandwidth, security \etc This request is received by the CPAC that will transmit it to the VN Embedding component for further treatment.
The CPAC also defines tenants' capacities to interact with their virtual network, which can be either API-based control or full control.

API-based control limits the capacities of the tenants by only exposing an API to interact with their slice in a specific manner. For instance, the tenant may only use the API of the hypervisor or must use a specific programming language to implement his applications for the hypervisor~\cite{CompositionalHypervisor-Jin2014,NetworkHypervisor-Huang2013}. 
The full control describes an hypervisor where the tenant can use its own application or SDN controller to interact with his virtual network. This solution is implemented by either slicing the infrastructure or maintaining a mapping between virtual and physical resources, as presented in the Section~\ref{sec:basic_def}.



\subsubsection{Data Plane Abstraction}
\label{sec:abstraction_comp}
The Data Plane Abstraction Component (DPAC) abstracts the physical infrastructure to serve each tenant with their own topology and resources.
This component is also an essential function of network virtualization and is implemented in every existing hypervisor.
It maintains the mapping between logical and physical topologies to translate logical decisions into physical ones.
% \GB{poor expression: the below paragraph should be rewritten}
It also makes sure that a tenant's operations on his Virtual Network does not interfere with the other tenants (\ie it maintains isolation between tenants). When a tenant deploys a new configuration in his network, such as routing protocols or ACLs, the DPAC is in charge of translating the virtual parameters and values used by the tenant into the corresponding ones for the physical infrastructure.

In practice, the abstraction can be done for several resources:

\subparagraph{\textbf{Topology}}\textbf{}\\
The hypervisor decouples the logical topology required by the tenant from the physical infrastructure.
Topology mapping can be either 1-to-1 or 1-to-many. 
The first mapping corresponds to the case where one virtual node (resp. link) is embedded in one physical node (resp. link), similarly to FlowVisor~\cite{FlowVisor-Sherwood2009} where the physical infrastructure is sliced and a small portion of it is presented to each tenant.
The second one embeds a single virtual node or link to multiple physical resources, as presented in~\cite{OpenVirteX-Al-Shabibi2014,VeRTIGO-Corin2012a}.
A particular approach is when the tenant only requires one virtual node to interconnect his VMs, and does not want to bear the burden of maintaining the configuration of each underlying node. In the literature, this approach is referred to as the ``Big Switch Abstraction".


\subparagraph{\textbf{Flowspace}}\textbf{}\\
% \GB{below sentence is too long: to be rewriten}
The flowspace of a virtual network defines which header fields and the range of values in the flowspace that a tenant can use in his addressing and routing schemes. This includes MAC addresses, IP addresses, transport layer ports, VLAN IDs, \etc
FlowVisor~\cite{FlowVisor-Sherwood2009} requires to share the flowspace among all tenants while other solutions, like OpenVirteX~\cite{OpenVirteX-Al-Shabibi2014}, offer a full address space virtualization.

\subparagraph{\textbf{Node Resources}}\textbf{}\\
The resources provided by a physical node are CPU power, and flow tables entries.
These resources are dissociated because they serve two distinct goals.
CPU provides computation power to process incoming packets and represents the forwarding capacity of the node.
Flow tables store the rules each tenant had inserted in the node.
Therefore one tenant can have high CPU percentage with a small flow table if the point is to switch multiple packets to a small set of sources and destination.
% The hypervisor level\GB{what is it? the term ``level'' is new! at least, illustrate with a figure}, including all the components required to achieve network virtualization, presents the tenant with an abstract view, and interacts with the physical infrastructure.

\subparagraph{\textbf{Link Resources}}\textbf{}\\
We consider here the bandwidth and the buffer available to process packets.
Abstracting the physical link resources is done by giving access to the bandwidth and the interfaces' buffers to the tenants.
As presented for the node resources, link resources can be allocated independently from one another.
For instance, a tenant may request large buffers to limit packets drop while another tenant may only requires a minimum bandwidth on each virtual link.

\subsubsection{Virtual Network Embedding Component}

The Virtual Network Embedding Component (VNEC) is in charge of determining the optimal set of physical resources to embed a virtual network and to handle the case where a virtual network must be migrated due to failure of a switch or because of an attack on the system.

\subparagraph{Virtual Network Embedding}
Virtual Network Embedding (VNE) is a resource allocation problem that can be solved using optimization techniques.
The use of a VNE algorithm to automatically deploy virtual networks is compulsory to avoid manual and impractical configuration operations.
The VNEC interacts with the Resource Isolation Component to check available resources so it can determine whether or not to accept the virtual network creation request.
The tenant's request is represented by a virtual network, and can include specific requirements such as minimum bandwidth, specific flow table  size, physical location constraints, \etc 
% When a tenant requests a virtual network, he describes his requirements in terms of virtual nodes, links, minimal bandwidth, security \etc 
This request is received by the CPAC that transmits it to the VNEC, to be processed.

The tenant may also specify whether or not his virtual network should be migrated if needed, and if a degraded mode is accepted during the recovery of normal operations.
The embedding of a virtual network can be constrained by the physical location of the underlying nodes.
Specific legislation may forbid confidential data from leaving the physical space of a country for instance.
On a similar aspect, a user may not want to share the same physical substrate with other tenants.
Distributed network hypervisors or multi-cloud hypervisor may be subject to such constraint since network equipments are geographically distributed.

% The set of physical resources may also be subject to location or security constraints.
% A standard user requirements can be either resources or placement.
% For instance, a user will need a certain amount of nodes, connected with a minimum bandwidth with the possibility to automatically migrate the topology in case of failures.
% More specific constraints would be the physical location of the topology.
% It can happen the traffic may not go through certain physical nodes.
% This placement problem is similar to virtual machine placement since there might be legal limitations to ensure the data is staying in a closed environment.

% Security constraints include the availability of cryptography to cipher the communication end to end, the possibility to refuse to host a virtual network on the same physical substrate or even the same datacenter that another virtual network etc.

\subparagraph{Virtual Network Migration}
% \GB{pleas swap the first two sentences. they may need to be adapted}
Changes in the infrastructure, failure on a link or a switch, or over-consumption of network resources may require a change in the mapping between the VN and the physical resources.
Therefore, in order to address these issues, it is possible to migrate the virtual network on a different subset of physical resources. 
The VNEC will be notified by the monitoring component, and then recompute the mapping of the topology with the physical infrastructure~\cite{VeRTIGO-Corin2012a,AutoSlice-Bozakov2012,CoVisor-Jin2015}.

% There are cases where the tenant should be notified about the migration.  
When a migration occurs, it might be necessary to notify the tenant about it.
For instance, depending on the Service Level Agreement (SLA) between a tenant and the infrastructure provider, the migration may not be feasible because the constraints cannot be satisfied by the current state of the infrastructure. This would require further instructions from the tenant.


\subsubsection{Resource Isolation}
The resource isolation component (RIC) ensures that the tenants are served the amount of resources they have requested during the exploitation of their VN.
% It ensures that each tenant does not exceed the amount of resources they have been allocated\GB{redundant with previous sentence}.
FlowVisor-based hypervisors~\cite{FlowVisor-Sherwood2009,ADVisor-Salvadori2012,VeRTIGO-Corin2012a,EnhancedFV-Min2012,SlicesIsolator-El-Azzab2011,DoubleFV-Yin2013} all implement bandwidth and CPU isolation, and some tackle advanced issues on resource sharing such as memory and interface isolation as summarized in Table~\ref{tab:existing-nhv}. One of the problems with implementing resource isolation is that the hypervisor relies on vendor specific implementations of these isolation features, which may lead to inconsistent behaviour of the hypervisor if the implementation varies between network equipments or if the feature is simply missing.


\subsubsection{Monitoring}
Monitoring the virtualization infrastructure serves two different purposes.
The first one is to support the proper functioning of the hypervisor's operations and is implemented on every hypervisor. However, it can range from switch discovery (similarly to an SDN controller) to resource monitoring and failure detection.
% \GB{does not belong to monitoring per se. it relies on another type of protocol (discovery)}
The second one is to serve the Security Component with data that will be used in detecting, preventing and mitigating attacks on VNs and the virtualization infrastructure.
Very few solutions implement a monitoring component to support VN migration or detect attacks~\cite{VeRTIGO-Corin2012a,CoVisor-Jin2015,FlowN-Drutskoy2012,AutoSlice-Bozakov2012,NVP-Koponen2014,ONVisor-Han2018}.

There are several types of information that may be monitored:

\subparagraph{Tenant to hypervisor traffic} Requests and configuration commands sent by a tenant can impact the security of the virtualization infrastructure~\cite{You2014,Costa2015}. A malicious tenant may exploit vulnerable hypervisors by sending forged requests that will alter legitimate users' virtual networks. The monitoring module may forward the tenant's requests to the Security Component for further investigation.

\subparagraph{Management traffic} 
The notifications sent by topology discovery protocols such as LLDP inform the hypervisor about the current state of the physical network. If a switch fails or becomes the target of an attack, the Security Component and the VNEC may be notified by the Monitoring.
% \GB{I would imagine that MON does not notify both SEC and VNEC, but that MON notifies SEC that notifies VNEC. especially if SEC has the responsibility to characterise whether an alert is indeed an attack or a false alarm}
The VNEC can then trigger a migration to relocate the virtual network on a new physical substrate or the Security Component will raise an alert and trigger mitigation procedures.
Management traffic also includes packets sent by and to physical equipments to ensure their proper operation.
This includes metrics about the resources used on each switch, information about the switch configuration or the errors raised.

\subparagraph{Configuration requests} An attacker may leverage vulnerabilities in switches or in the authentication scheme to deploy malicious configuration rules. These rules may alter the behavior of the networking equipments and may lead to a data exfiltration scenario or a Denial of Service.
The Monitoring may forward configuration rules to the Security Component for detection purposes we describe in the next component.

\subsubsection{Security Component}
The Security Component (SC) is in charge of detecting, preventing and mitigating attacks on the virtualization infrastructure. The SC relies on the monitoring of the infrastructure to collect useful data related to the security of the virtual networks and the physical equipments.
The problem of the security inside a network hypervisor has only been partially studied~\cite{Costa2015}, but not in the context of the migration of virtual networks.
% The SC can operate on several levels, namely detection, prevention and mitigation.

\subparagraph{Detection}
The detection of attacks relies on networking events and tenant inputs.
For instance, the number of requests sent by the tenant to the hypervisor may be monitored in order to detect a flooding behavior that would lead to an overload of the system. Another fitting example is to examine the content of configuration rules and match this content against a set of security rules (\eg the Wildcard Rewrite problem~\cite{Costa2015}).
There is so far no proposition of an attack detection mechanism for network hypervisors.

\subparagraph{Prevention}
Prevention of attacks can be implemented by either limiting the attack surface accessible to an attacker or by preventing the attack to reach its destination. The limitation of the attack surface is proposed by CoVisor~\cite{CoVisor-Jin2015} where each tenant is limited in its ability to handle network traffic.
NVP~\cite{NVP-Koponen2014} implements tunnels between virtual nodes to leverage tunnel encryption.
ONVisor~\cite{ONVisor-Han2018} implements an access control module in charge of authenticating tenants applications and preventing them from interfering with other virtual networks.
Prevention may also be enforced by deploying Virtual Network Functions~\cite{vnf} in the infrastructure. This aspect is not well exploited because attack detection on the virtualization infrastructure is not well investigated. 

\subparagraph{Mitigation}
Depending on the type of attack, the mitigation can take several forms. 
In case of a DoS attack, the security component would rely on the VNEC to provide an adequate migration scheme but it can also improve the performance of the VNEC by leveraging security techniques to determine a better substrate, like heuristics to determine which physical nodes are less likely to be targeted. 
% If the configuration of physical nodes has been altered, the redeployment of configuration rules could take into account the location of the attacker.

\subsection{Workflows in the reference architecture}
% \GB{I wonder why all flows are not bidirectional. if not, maybe the graph in Fig.~\ref{fig:reference-archi-nh} should be oriented}
In this section we describe the different information flows illustrated by Figure~\ref{fig:reference-archi-nh}.

\circled{1} Tenant~$\leftrightarrow$~CPAC: This flow is mainly composed of requests sent by the tenant and the view of the virtual network sent by the hypervisor. Additional traffic may include specific notifications (\eg security and performance alerts).

\circled{2} CPAC~$\leftrightarrow$~VNEC: The CPAC transmits requests for new virtual networks to be deployed on the physical infrastructure, and the VNEC returns whether or not the request is accepted.

\circled{3} CPAC~$\leftrightarrow$~Monitoring: The CPAC forwards requests sent by the tenant to the monitoring module, for logging and security purposes. 

\circled{4} CPAC~$\leftrightarrow$~DPAC: The CPAC forwards flow rules from the tenant to the DPAC that will translate virtual identifiers into the physical ones and deploy these rules on the physical infrastructure.

\circled{5} Security~$\leftrightarrow$~CPAC: The security component analyzes networking events and tenants requests and returns alerts.

\circled{6} VNEC~$\leftrightarrow$~RIC: The VNEC interacts with the RIC to ensure the availability of resources to compute the embedding of the requested virtual network.

\circled{7} Monitoring~$\leftrightarrow$~VNEC: The monitoring component transmits to the VNEC a notification for migration in case of a failure in the infrastructure.

\circled{8} VNEC~$\leftrightarrow$~DPAC: The VNEC transmits to the DPAC the flow rules corresponding to the virtual network that must be deployed in the infrastructure, whether it is a new request from the CPAC or a migration.

\circled{9} Monitoring~$\leftrightarrow$~Security: The monitoring forwards to the security component suspicious requests or networking events for further processing.

\circled{10} RIC~$\leftrightarrow$~Monitoring: This flow is composed of performance metrics used by the RIC to verify the proper isolation of resources in the infrastructure.

\circled{11} RIC~$\leftrightarrow$~DPAC: This flow represents the countermeasures sent by the RIC to the DPAC to ensure the proper isolation of resources in the infrastructure.

\circled{12} Monitoring~$\leftrightarrow$~Infrastructure: This flow is composed of topology discovery messages, performance metrics and networking events collected in the infrastructure.

\circled{13} DPAC~$\leftrightarrow$~Security: The security component sends to the DPAC the counter-measures to be deployed in the infrastructure.

\circled{14} DPAC~$\leftrightarrow$~Infrastructure: This flow is composed of flow rules deployed by the DPAC on the physical nodes.
% and networking messages sent by the infrastructure (\eg topology discovery, node failure \etc.).

% \subsubsection{Workflows}
% In this section, we describe how the different components interacts together.
% There are several scenarios presented, initial deployment, migration.
% %\FC{Add scenarios if needed}
% Figure~\ref{fig:Workflow} represents the interactions and interfaces of these processes.
% \subparagraph{\textbf{Initial Deployment}}\textbf{}\\
% In a first time, the tenant will use the configuration component to describe the topology and the resources needed, as well as the different authorizations for communicating with other tenants' topologies.
% The output of this component would be a configuration file.
% Then, this file will be transmitted to the abstraction component (AC), the multi-tenants communication component (MCC) and the VNE Component.
% There are three simultaneous steps.
% The abstraction component will store the resources requested by the users.
% The MCC will store the description of the rights given by the user to the other tenants.
% The configuration file will be analyzed by the VNE component in correlation with the MCC and  the AC and will decide, if possible, which physical nodes are best suited to host this topology, according to the resource allocation policy.
% At this point, the hypervisor knows which nodes and which links will be used for this topology.
% The mapping computed by the VNE will be stored in the AC so it can process user requests.
% Therefore, these information can be transmitted to the VNI component that will push the different flow rules into the nodes.
% However, in case the requested topology cannot fit in the infrastructure, due to lack of resources, inter-tenants communication requests etc., the VNE component must return an error to the user.

% \begin{figure}[h]
% \centering
% \includegraphics[scale=0.7]{figures/network-hypervisor-workflow}
% \caption{Deploying a topology in a network hypervisor.\label{fig:Workflow}}
% \end{figure}

% \subparagraph{\textbf{Migration}}\textbf{}\\
% The migration component collects alerts from the infrastructure about failures or resource congestion.
% Figure~\ref{fig:Workflow} illustrates how the component receives an infrastructure alert and transmits it to the VNE component so he can recompute the topologies.

% During runtime, a user may push a request for resources.
% This request is transmitted to the RAC for verification.
% If the request can be granted, the RAC will notify the VNI to deploy the resources.
% Oherwise, an error should be raised and returned to the user.




\paragraph{Definitions}
A Markov Decision Process~\cite{bellman1957} (MDP) is a mathematical model used to describe the interactions  an agent may have with a system.
The system is described by the set of state it can be in, and when in a particular state the agent will choose an action (interaction) to perform.
Each action is associated to a potentially non null reward, considered as an incentive for the user.
However, the outcome of each action is probabilistic thus incurring a certain amount of uncertainty about how the system will evolve.
Therefore, after choosing an action the system may transition towards one out of several states (or remain in the same state), according to a probability distribution.



\textbf{\\}
Formally, a MDP is defined by a 4-tuple $<S,A,P(s,s',a),R(s,a,s')>$ where:
\begin{itemize}
    \item $S$ is a finite set of states
    \item $A$ is a finite set of actions
    \item $P(s,s',a) : S \times A \times S \longrightarrow [0,1]$ is a transition probability function
    \item $R(s,a,s') : S \times A \times S \longrightarrow \mathbb{R}$ is a reward function
\end{itemize}

The aim of an MDP is to determine an optimal policy, defining for each possible state which action will maximize the reward gained by the user while accounting for the future consequences of an immediate choice.


\paragraph{Solving an MDP}
\label{sec:optimalpolicy}
Before determining the optimal policy for a MDP, we have to dive into the notion of reward of an MDP. Any action chosen in a specific state may reward the agent. However, the choice made now may also lead the agent to negative consequences that will not compensate for the immediate reward obtained. 
This kind of drawback is mitigated with the notion of a long term reward, called utility. The utility of a state is the immediate reward gained when entering this state plus the rewards that can be obtained from reachable states.
These future rewards are impacted with a discount factor, because the principle of uncertainty makes them less attractive than the immediate reward.
With all these considerations, we can now determine the optimal policy for the defender.
The solution of an MDP consists in determining the action that will maximize the immediate and future rewards for each existing state.

\subparagraph{The Bellman equation}
Bellman proposes in~\cite{bellman1957} an analysis of equations that will help in decision making using a MDP. 
More precisely, the determination of the maximum reward expected in an MDP is expressed as non-linear equations, however he proves that they possess quasi-linear properties that allow us to solve these equations using methods only applicable to linear cases. 
The Bellman equation shown in Fig.~\ref{fig:bellmaneq} lays the groundwork for solving MDPs using dynamic programming.
This equation states that the optimal utility $U(s)$ expected from a state $s$ is the sum of the reward obtained when entering state $s$ and the expected discounted sum of probable utilities of the neighbours of $s$. $\gamma$ represents the discount factor for the future reward.

\textbf{Vocabulary and notations precisions}\\
$R(s,a,s')$ is sometimes simplified to $R(s,a)$ in the literature.\\
The term value is sometimes used interchangeably with utility.

\begin{figure}[h]
\[ U(s) = \max\limits_a \left \{R(s,a) + \gamma   \sum\limits_{s'} P(s,a,s')U(s') \right \} \]


\caption{The Bellman's equation}
\label{fig:bellmaneq}
\end{figure}

The solution of a MDP is the optimal policy $\pi(s)$, which is a vector defining the optimal action for each possible state in the MDP.
There are three main algorithms to determin the optimal policy: Value Iteration, Policy Iteration and Q-Learning.
We detail those algorithms hereafter.
% There are several algorithms to determine the optimal policy, notably Value/Policy Iteration and Q-Learning.
% We will focus on Value Iteration because Q-Learning is applied to systems where the transition model is unknown, and Policy Iteration is very similar to Value Iteration.


\subparagraph{Value Iteration Algorithm}
This algorithm~\cite{bellman1957} takes an iterative approach to determine the value of each state in the MDP with the Bellman equation.
Value Iteration works as follows:
\begin{enumerate}
    \item Initialization of a value vector $V_0(s)=0,~\forall s\in S$
    \item Determine the utility of each state iteratively\\{$V_{k+1}(s) = \max\limits_a \left \{ P(s,a,s')[R(s,a,s') + \gamma \sum\limits_{s'}V_k(s')] \right \}$}
    \item Repeat step 2 until $V_{k}(s)$ converges
    \item Compute the optimal policy $\pi(s)$\\$\pi(s) = \argmax\limits_a \left \{P(s,a,s')[R(s,a) + \gamma \sum\limits_{s'}V(s')] \right \}$
\end{enumerate}

\subparagraph{Policy Iteration Algorithm}
Similarly to the previous algorithm, the Policy Iteration algorithm~\cite{policyiteration} works iteratively but directly determines the optimal policy instead of the value of the states.
Policy Iteration works as follows:
\begin{enumerate}
    \item Initialize of value vector and policy vector $~\forall s\in S,\pi(s)=a\in A,~U(s)=0$
    \item Policy evaluation
        \begin{algorithm}
            \Repeat{$\Delta < \Theta$ (a small positive number)}{
                $\Delta = 0$\\
                \ForEach{$s\in S$}{
                    $temp = U(s)$\\
                    $U(s) = P(s,a,s')[R(s,a,s') + \gamma \sum_{s'}U(s')]$\\
                    $\Delta = max(\Delta,|temp-U(s)|)$
                }
            }
        \end{algorithm}
    \item Policy improvement 
    \begin{algorithm}
        stable-policy = True
        \ForEach{$s\in S$}{
            $temp = \pi(s)$\\
            $\pi(s) = \argmax\limits_a \left \{ P(s,a,s')[R(s,a,s') + \gamma \sum\limits_{s'}V(s')] \right \}$
            \uIf{$temp \neq \pi(s)$}{stable-policy=False}
        }
        \uIf{stable-policy}{exit}
        \uElse{goto Step 2}
    \end{algorithm}
\end{enumerate}

The computational complexity of both algorithms is studied in~\cite{mdpcomplexity}.

\subparagraph{Q-Learning}
The Q-Learning algorithm~\cite{qlearning} is used when the MDP considered does not provide an explicit definition of its model (\ie transitions and/or rewards).
The agent only knows what state he's in, and upon choosing an action the agent will observe a reward for transitioning from state $s_t$ to $s_{t+1}$.
The Q function defines the \textit{quality} of action $a$ at state $s_t$.
Similarly to the Value Iteration algorithm, updating the Q function starts with an arbitrary initialization and will be gradually updated as time passes (See Equation~\ref{eq:updatingq}).

\begin{figure}[h]
\begin{equation}
Q'(s_{t},a_{t}) \leftarrow (1-\alpha) \cdot \underbrace{Q(s_{t},a_{t})}_{\text{old value}} + \underbrace{\alpha}_{\text{learning rate}} \cdot  \overbrace{\bigg( \underbrace{r(s_t,s_{t+1})}_{\text{observed reward}} + \underbrace{\gamma \max_{a}Q(s_{t+1}, a)}_{\text{estimate of optimal discounted value}} \bigg) }^{\text{learned value}}
    \label{eq:updatingq}
\end{equation}
\caption{Updating the Q function}
\end{figure}

\paragraph{}
\cite{Anupama2014,Wang2013,Liang2011,Robelin2007,Liang2012,Tesauro2006,Bokani2015,Chades2014}
\label{sec:mdp-conclusion}
In this chapter, we investigated the Resource Allocation problem in a security context. Our objective was to determine which nodes of the infrastructure should be performing monitoring tasks to detect attacks against the migration process.
We proposed a Markovian Decision Process to represent how an infrastructure provider can solve this RA problem and find out the optimal choices.
We introduced the attacker in the definition of the model, which is rarely done from a security perspective. Indeed, MDPs are often modeling only one agent to interact with the system, but the inclusion of an actor of a different nature is generally not considered.
We also provided an experimental prototype for the MDP generation, solving and results analysis  (\url{https://github.com/FabienCharmet/MDPRA}).
Results show that we can determine which nodes provide the best security with regard to current attacks, as well as how the dynamic aspect of the optimal policy can be translated into an \textit{a priori} deployment of the monitoring resources on the nodes.
When the attacker can launch attacks from several sources, the impact of the nodes on the monitoring is much more differentiated and gives a better understanding of their role in the infrastructure.
We also ran the simulations on several network topologies to evaluate the impact of the attack routing on the optimal solution. Results show that a full-mesh topology is more complex to defend because of the multitude of attack paths.
% Markov Decision Processes are rarely used for a resource allocation problem in a security context.

We advocate that the flexibility of the MDP formalism can be leveraged to dynamically recompute the destination substrate while the migration is ongoing.
Instead of deciding where to deploy the monitoring, the migration process would use the MDP to predict the target of the next attack and should recompute the embedding of the VN if the next virtual node will be embedded on a compromised physical node. The approach would give additional flexibility to the VNE algorithm and make it more secure. We emphasize on the Reinforcement Learning aspect of this approach, which could be extended by using a Q-Learning model where the defender does not know precisely which nodes have been attacked.
% \GB{pour \'eviter de rester dans des d\'ebats st\'eriles, je cite de nouveau Wikipedia (ham): ``The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible''. Donc le type de MDP que tu proposes n'est pas du RL.}\FC{Tout Ã  fait d'accord dans ce cas la.}


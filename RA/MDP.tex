\section{Markov Decision Process for Resource Allocation}

Markov Decision Processes are used to represent how a defender may choose network nodes to support the monitoring.
While the attacker is not directly represented as an agent that will interact with the infrastructure, he can be represented in the definition of the states of the MDP and in the rewards obtained by the defender.
In this chapter, we model the migration of a Virtual Network with an MDP and represent an attacker compromising the migration process.
The solution of an MDP is a dynamic strategy that the defender should follow depending on the current state of the infrastructure (\ie the optimal policy).
We intend to convert the dynamic aspect of this policy into an \textit{a priori} deployment of the monitoring resources (\ie deploying the resources prior to the migration of the Virtual Networks).
While our MDP model could be applied to traditional networks, we have designed an attack model exploiting specificities of the SDN paradigm. 
An experimental prototype is available on github\footnote{\label{github}\url{https://github.com/FabienCharmet/MDPRA}}.



% \input{RA/RA-MDP/MDP-Requirements.tex}
\input{figures/exfiltration.tex}


\subsection{Infrastructure assumptions}
\input{RA/RA-MDP/MDP-assumption.tex}

\input{figures/migration_trigger.tex}
\subsection{Attacker Model}
\input{RA/RA-MDP/MDP-attackmodel.tex}

\subsection{Modeling the RA problem with an MDP}
\input{RA/RA-MDP/MDP-MDP.tex}

\newpage
\subsection{Generating the states of the MDP}
% \thispagestyle{empty}
\input{RA/RA-MDP/MDP-genstate.tex} 

% \FC{Don't forget to hide page number 113}
\newpage
\subsection{Use Cases}
\input{RA/RA-MDP/MDP-usecase.tex}

\subsection{Determining the optimal monitoring nodes}
\input{RA/RA-MDP/MDP-optimal-monitoring.tex}

\subsection{Discussion}
% \GB{you do not discuss the complexity limitation of your MDP. I think it is important and honest to point it out.}
\input{RA/RA-MDP/MDP-discussion.tex}

\section{General Conclusion}
\input{RA/RA-MDP/MDP-conclusion.tex}
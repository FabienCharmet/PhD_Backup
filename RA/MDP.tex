\input{figures/migration_trigger.tex}

\subsubsection{Attacker model}
\label{sec:attack_model}
Fig.~\ref{fig:trigger} depicts the evolution of the infrastructure once the attacker makes part of it unavailable. 
At first, the virtual network is running on a healthy physical substrate; but once the attack is launched, the substrate becomes unavailable and the virtual network must be migrated quickly to reduce the end user's service interruption.
The success of the attacks on the migration relies on two main aspects: the ability to affect the configuration of SDN nodes as well as  to retrieve the exfiltrated data.
While the latter can be easily solved by owning virtual machines in the infrastructure, the former requires to be able to alter nodes configuration. This has been proven possible in~\cite{Taxonomy_Hizver2015, Bokani2015, attain-Ujcich2017}.
Precisely, the attacker is able to spoof the identity of the network hypervisor, and thus is able to inject malicious flow rules inside the nodes to create the data exfiltration path. 
However, he is not able to be designated as the original network hypervisor in the nodes' configurations. 
% \GB{not completely clear}\FC{Better ?}. 
This can be explained because it requires advanced configuration privileges. Moreover, a physical node missing from the legitimate hypervisor's topology view is easily detectable, in comparison to malicious flow rules injected inside the physical nodes.
% \FC{If keeping this part we will need an introduction to SDN, probably will need one anyway}\FC{This can be explained because it requires advanced configuration privileges, and moreover, a physical node missing is easily detectable by the legitimate controller compared to a misconfiguration on a swi/attacking switch, not proxifying the communication}
Even though the virtualization infrastructure hosts several virtual networks and end users, we limit the scope of the attacker to a unique target virtual network. 
% The attacker is motivated to attack a particular user inside the infrastructure, thus targeting one particular virtual network. 
He has been able to determine which nodes to attack to trigger the migration thanks to prior scanning and information gathering. 
% ~\GB{you mean ``reconnaissance'' and ``scanning''}
Nevertheless, he has no exact knowledge about which nodes will be selected as the destination substrate and he will discover it by doing further scanning and fingerprinting  while he is attacking the infrastructure.  
Even if the attacker may target all nodes in the infrastructure, he has no incentives to attack nodes that will not contribute to exfiltrate data from his victim's network. 
% \GB{so to identify them, the attacker has to have some global visibility, which she gets from scanning. Is it feasible?}

% \GB{you already said that earlier. Can you be more specific now?}\FC{Aren't the provided sources enough ?}
We can find a description of such techniques in~\cite{Hong2015,Sphinx-Dhawan2015}.
This information gathering is not depicted in this paper and we consider that the attacker will choose his targets as presented in Section~\ref{sec:target_proba}.
From the point of view of the defender, it is impossible to accurately know which node will be attacked.
% in the infrastructure.
% \GB{of the future substrate? or any physical node from the infrastructure?}.
The attacker may own several virtual machines inside the infrastructure, thus has several sources to launch an attack. However, he can only attack one node at a time. 
We suppose that each node will always be attacked from the same source. 
Because of the short time interval considered for the migration, we suppose that the attack will always take the same path. 
This path will be considered to determine the global detection probability of the attack.
% , since all the nodes monitoring the path may see the attack come through.
% \GB{so this is very important to understand at what level is located the attacker. Is she another tenant? Or does she have access to the substrate? Do attack packets flow at the data plane or at the control plane?}.\FC{The attack is routed in the control plane but impacts the data plane. I don't want to go to deep in details so we do not add too much confusion. We can simplify by saying that the control plane topology is the same as the data plane topology.}


\input{figures/exfiltration.tex}

% \GB{we know the attack lasts a little longer but how long does it make sense to monitor or simply to model?}
% \FC{With this model, the attacker may attack as long as there is budget to spend. The migration will have already ended. We suppose we can monitor more nodes than the number of migrated VNodes}



\subsubsection{Assumptions}
\label{sec:hypotheses}
In this section we describe the assumptions we make to address the problem statement.

\paragraph{Migration}
All nodes in the original substrate have been fully compromised by the attacker and thus are not considered as candidates for the destination substrate.  
This assumption is reinforced by the fact that forcing all the resources to be reallocated could be leveraged by the attacker in an attempt to have the target virtual network relocated closer to his virtual machines.
Virtual machines are already subject to such attacks, as presented by Atya \etal in~\cite{stalling-atya2017,malicious-atya2017}.
The migration will deploy in the destination substrate all the flow rules necessary to operate the virtual network properly. The migration of the virtual nodes is sequential~\cite{Lime-Ghorbani2014}, thus all the nodes will be migrated one at a time.
We suppose that both virtual network and physical infrastructures are static (\ie the topology does not change over time).

\paragraph{Monitoring}
% \GB{you could call it ``Monitoring''}
The deployment of the monitoring on the nodes impacts the defender financially and the infrastructure's performance. Based on the work of Ismail \etal~\cite{interdep-ismail2017}, we consider the monitoring cost  proportional to the intrinsic value of the nodes, (\eg CPU time on a powerful machine is more expensive compared to a smaller one). 
% The performance impact is considered uniform on all the nodes. 
Each node on the path of an attack has the same probability to detect it, \eg there is no node more efficient than another. 


\paragraph{Targeting nodes}
\label{sec:attacking}
% \GB{one of the main assumptions is therefore that the attack path is in the substrate}
% \FC{The attack path is from attacker to target node. However, the other path, the path used to exfiltrate data will be in the substrate obviously. I will differentiate these two types of paths}
During the migration, the attacker may target nodes to construct the path that will support the exfiltration of the information.
We make the assumption that substrate nodes are more likely to be attacked since at least one must be part of the path leading to the exfiltration point.
The attacker's strategy for choosing which nodes he attacks is based on the information gathering he performs while attacking. The details of such activity is considered out of the scope of this paper.
Similar work on cloud environments for virtual machines colocation has been proposed in~\cite{getoffmucloud-Ristenpart2009, incentivemtd-Zhang2012}.
% \GB{not sure of this term being used in the literature. What about co-location or even co-residency?} 
% \FC{Correct spelling seems to be collocation but literature says colocation, let's go}
Johnson \etal propose in~\cite{mitigateAPT-johnson2013} a real time metric that determines the node that is the most likely to be the next target of an attack.
If the attacker was able to establish the full path then we consider that the global attack was successful.

\subsubsection{Background of Markov Decision Processes}
\label{sec:mdp}
In this section we give a formal definition of a Markov Decision Process (MDP) and present a common algorithm to solve it.

\paragraph{Definition}
%The Markov Decision Process is a model used to describe a system on which an agent (\ie the defender) will perform specific actions.
An MDP is a formalism used to represent the evolution of a system based on the decisions made by an agent.
These decisions may reward the agent and cause the system to evolve, according to a certain transition probability function.
% The formalism of an MDP is used to represent the actions performed by an agent on a system. The agent may be rewarded upon choosing an action, and such action may cause the system to evolve.
Formally, a Markov Decision Process is defined by a 4-tuple $<S,A,P(s,s',a),R(s,a,s')>$ where $S$ is a finite set of states, $A$ is a finite set of actions, $P(s,s',a)$ is a transition probability function to go from state $s$ to $s'$ when choosing action $a \in A$ and finally $R(s,a,s')$ is the immediate reward gained by choosing $a$ and transitioning from state $s$ to $s'$. $R(s,a,s')$ is sometimes simplified to $R(s,a)$ in the literature.
Actions may reward the agent and can lead the system to evolve from its current state to another.
% The MDP is a model representing  the different actions a user can make on a system over a certain period of time.
% Each action is associated with a potentially non null reward, considered as an incentive for the user.
% However, the outcome of each action is probabilistic thus incurring a certain amount of uncertainty about how the system will evolve.
% Therefore, after choosing an action the system may transition towards one out of several states (or remain in the same state), according to a probability distribution.
The aim of an MDP is to determine an optimal policy, defining for each state which action will maximize the reward gained by the user while accounting for the consequences of an immediate choice.
% \GB{does ordering affect the results? or just the specific subset of nodes selected?}\FC{Ordering matters, but we aim to overcome that with a priori deployment}
% Attacker strategies represent which nodes will be attacked during the migration, and are taken into account from the defender's point of view.


\paragraph{Solving an MDP}
\label{sec:optimalpolicy}
% Before determining the optimal policy the defender should follow to deploy the monitoring inside the infrastructure, we have to dive into the notion of reward of an MDP. Any action chosen in a specific state may reward the agent. However, the choice made now may also lead the agent to consequences that will not compensate for the immediate reward obtained. 
% This kind of drawback is mitigated with the notion of a long term reward, called utility. The utility of a state is the immediate reward gained when entering this state plus the rewards that can be obtained from reachable states.
% These future rewards are impacted with a discount factor, because the principle of uncertainty makes them less attractive than the immediate reward.
% From all these considerations, we can now determine the optimal policy for the defender.
The solution of an MDP consists in determining the action that will maximize the immediate and future rewards for each existing state.

\subparagraph{The Bellman equation}
% In 1957, Bellman proposes in~\cite{bellman1957} an analysis of equations that will help in decision making using MDPs. 
% More precisely, the determination of the maximum reward expected in an MDP is expressed as non-linear equations, however he proves that they possess quasi-linear properties that allow us to solve these equations. 
The Bellman equation~\cite{bellman1957}, shown in Fig.~\ref{fig:bellmaneq} lays the groundwork for solving MDPs using dynamic programming.



This equation states that the optimal utility expected from state $s$ is the sum of the reward obtained when entering state $s$ and the expected discounted sum of probable utilities of the neighbours of $s$. $\gamma$ represents the discount factor for the future reward.

\begin{figure}[h]
\[ U(s) = \max\limits_a \left \{R(s,a) + \gamma   \sum\limits_{s'} P(s,a,s')U(s') \right \} \]

\setlength\abovecaptionskip{-2ex}
\setlength\belowcaptionskip{-3ex}
\caption{Bellman's equation}
\label{fig:bellmaneq}
\end{figure}

The solution of a MDP is the optimal policy $\pi(s)$, which is a vector defining the optimal action for each possible state in the MDP.
We propose to determine the optimal policy using the well known Value Iteration algorithm.
% There are several algorithms to determine the optimal policy, notably Value/Policy Iteration and Q-Learning.
% We will focus on Value Iteration because Q-Learning is applied to systems where the transition model is unknown, and Policy Iteration is very similar to Value Iteration.

\subparagraph{Value Iteration Algorithm}
This algorithm takes an iterative approach to determine the value of each state in the MDP with the Bellman equation.
Value Iteration works as follows:
\begin{enumerate}
    \item Initialize a value vector $U_0(s)=0,~\forall s\in S$
    \item Determine the utility of each state iteratively\\\makebox[22em]{$U_{k+1}(s) = \max\limits_a \left \{R(s,a) + \gamma \sum\limits_{s'} P(s,a,s')U_k(s') \right \}$}
    \item Repeat step 2 until $U_{k}(s)$ converges
    \item Compute the optimal policy $\pi(s)$\\\makebox[21em]{$\pi(s) = \argmax\limits_a \left \{R(s,a) + \gamma \sum\limits_{s'} P(s,a,s')U(s') \right \}$} 
\end{enumerate}

\subsubsection{Model}
\label{sec:model}
In this section, we describe our MDP model to address the problem of optimal defense resource allocation for virtual network migration.

\paragraph{States}
\label{sec:stateset}
The states in the MDP represent the evolution of the system, the progress of the migration, the remaining budgets as well as the compromised nodes in the infrastructure.
The defender has two different budgets: $b_f$, the financial budget for setting and maintaining monitoring, and $b_c$ the global computational power available for the monitoring.
Precisely, $b_c$ corresponds to the overall performance impact caused by the monitoring. % the defender is willing to perform.
%  on the network virtualization service
 This represents the amount of resources that can be spent to perform monitoring instead of operational tasks.

% \GB{is $b_c = 0$ a halt condition?}\FC{No, monitoring actions are just not available, and choosing them only cost budget with no reward}.

%GB redundancy detected, so I commented out the following lines
We describe the state of the system as the following tuple: $s=<b_f,b_c,Mi,Mo,At>$:
\begin{itemize}
    \item $n$: The number of nodes in the infrastructure.
    \item $\textbf{N} = \{1,..,n\}$ is the set of nodes in the infrastructure.
    \item $Mi^s \subset \textbf{N} $ is the set of currently migrated nodes for state $s\in S$.
    \item $Mo^s \subset \textbf{N}$ is the set of currently monitored nodes for state $s\in S$.
    \item $At^s \subset \textbf{N}$ is the set of compromised nodes for state $s \in S$.
    \item $b_f^s$ is the remaining financial budget of state $s$.
    \item $b_c^s$ is the remaining computational power of state $s$.
\end{itemize}

An absorbing state is a state where all actions transition back to itself.

\paragraph{Actions}
\label{sec:actionset}
The defender can either add monitoring on a particular node in the infrastructure, remove this monitoring, or choose to do nothing.
The option of doing nothing prevents counter-productive options like forcing undesired actions (\eg unjustified unmonitoring).
We note $m_j$ the action of setting up monitoring on node $j$. Similarly, we note $u_j$ the action of removing monitoring on node $j$.
Finally we note $d$ the action of doing nothing.
%\\With $n$ being the number of nodes in the infrastructure 
\\We define $A = \{m_1,..,m_n,u_1,..,u_n,d\}$ as the set of actions available at each state.
% \GB{you really need to defined this term. What does it involve?}
% \FC{What do you think about that ?}

% \FC{The last node added to the path should be the node belonging to $\mathbb{Z}$ because otherwise exfiltrating data without the full path will raise a lot of unexpected $packet-in$ . This may not be an issue though}
% We also assume he may construct the full path by connecting nodes one after another instead of randomly constructing the chain. 

\paragraph{Probabilistic target determination}
\label{sec:target_proba}
We consider the path taken by the attack as a set of nodes.
% We name $L$ the set of paths inside the infrastructure. 
We define $L_j$ the path leading the attacker to node $j$.
We note $\textbf{Z}$ the set of non compromised nodes that are currently embedding the migrated virtual network (\ie $\textbf{Z} = Mi^s \cap \overline{At^s})$.
Similarly, we note $\textbf{T}$ the set of non compromised nodes that are directly connected to a compromised node and are not part of $\textbf{Z}$, \ie they are potential candidates for the path between attacker and victim. Alg.~\ref{algo:target} is computed at each state as the sets $\textbf{T}$ and $\textbf{Z}$ are changing.
% This implies that compromising a node in $\mathbb{T}$
% \FC{Add algorithm for function q to determine targets}
\begin{algorithm}[]
 \SetKwInOut{Input}{Input}
 \SetKwInOut{Output}{Output}
 \Input{\textbf{Z}, \textbf{T}, current state $s$, \textit{n} nodes}
 \Output{\textit{n} probabilities to attack each of \textit{n} nodes}
 initialization\;
 \ForEach{node $j$ in \{$1,..,n$\}}{
  \uIf{$j \in \textbf{Z}$ and $Mi^s \cap At^s = \{\emptyset \}$}{
   $q(j) = \alpha  \frac{3}{3|\textbf{Z}| + |\textbf{T}|}$
   }
  \uElseIf{$j \in \textbf{T}$} {
   $q(j) = \alpha  \frac{1}{3|\textbf{Z}| + |\textbf{T}|}$
  }
  \Else{
  $q(j)=0$
  }
 }
 \caption{Probabilistic target determination}
 \label{algo:target}
\end{algorithm}


We note the probability of a specific node being attacked as the combination of the probability for an attack to be launched (\ie $\alpha$) and the probability of the node being chosen among all the nodes in $\textbf{T}$ and $\textbf{Z}$. We also use a coefficient (here 3) to give more weight to the nodes in $\textbf{Z}$, referring to the assumption made in~\ref{sec:attacking}.
Once a substrate node has been compromised, the attacker will finalize the exfiltration set-up by completing the path between his VM and the victim's network.


\paragraph{Transitions}
We describe the coherence of the MDP states with the following transition constraints:

\begin{enumerate}
    \item If there is no $b_f$ budget left, a state cannot transition to another state (absorbing state)
    \label{cond:c1}
    \item Choosing an action without the required $b_c$ budget will only cost $b_f$ budget with no reward
    \label{cond:c2}
    \item As long as there are nodes to be migrated, each transition will include the migration of one node.
    \label{cond:c3}
    \item A state can only transition to states that preserve the coherence of parameters (budgets, etc.)
    \label{cond:c4}
    \item If there is an action too expensive for $b_f$ it will consume the remaining $b_f$ without any reward
    \label{cond:c5}
    \item Choosing an action twice (\ie monitoring a node already monitored) only consumes $b_f$ with no reward
    \label{cond:c6}
    % \FC{This is like doing nothing, maybe it shouldn't transition to another state and just transition back to current state with no reward}
    % \GB{does double monitoring make sense?}
    % \FC{No, it's just a formalism constraint}
 \end{enumerate}

Each node $j \in \textbf{N}$ is characterized by an intrinsic value $V_j \in \mathbb{R}$ that can be seen as the financial value of the node, its computing capacities and its function inside the virtualization infrastructure. 
%We note $\{V_1,..,V_n\}, \forall i~V_i \in \mathbb{R}$ as the set of said values, one for each node. 
Considering that the monitoring cost for each node is not uniform, we note $k_{f}$ and $k_{c}$ the atomic monitoring costs respectively associated to budgets $b_f$ and $b_c$.
Then we define the monitoring costs $c_f^j$ and $c_c^j$ for node $j$ as follows:

\begin{equation}
\forall j \in \textbf{N}, c_f^j = k_{f}  V_j,~ c_c^j = k_{c}  V_j\\ 
\end{equation}
% \GB{are these atomic costs constant? If so, it means that the monitoring costs $c_f^j$ and $c_c^j$ are related as they both rely on $V_j$. Does it make sense to have two distinct costs anymore?}.
% \FC{Yes because we consid= maxer a depleting resource and a non depleting one. Those budgets are of different nature}
% Budget $b_f$ is impacted at each transition, where the cost of each monitoring node will be deducted.
% Budget $b_c$ is impacted only at the set up or removal of monitoring.
When in state $s$ and after choosing an action $a \in A$, the system can transition to $|\textbf{Z}|+|\textbf{T}| + 1 $ states, whether an attack happened on one of the nodes or no attack was launched.
We define $S' \subset S$ the set of states to which the state $s$ can transition to with a non null probability.
We note $s'_{a,j} \in S'$ the state depending on which action $a$ has been chosen and which node will be attacked (index $j$), and if no attack was launched we note $s'_a \in S'$. To ease the reading we simplify $s'_{a,j}$ and $s'_a$ to $s'$ for the rest of this paper.
We define the state modifications once action $a \in A$ has been chosen.
% \GB{I assume $S' \in S$, amirite?}.
% We define $s'_{j} \in S'$ as the state where node $j$ has been attacked and $s'_0$ as the state where no attack happened.
% \GB{what about these below impact computations? how come a state equals a substraction of a cost from another state...}
% \FC{I rewrote the definition of an attribute $s.b_c$ is the $b_c$ budget related to s, see section~\ref{sec:stateset}}
\\
\paragraph*{\textbf{State modifications for action $m_i$}}
when choosing action $m_i$ at state $s$, we compute the budget impact and set changes for state $s', \forall j \in \textbf{N}$. 

\begin{equation}
  s \longrightarrow s' =\begin{cases}
    b_f^{s'} = b_f^s - c_f^i\\
    b_c^{s'} = b_c^s - c_c^i\\
    Mo^{s'} = Mo^s \cup \{i\}\\
    At^{s'} = At^s \cup \{j\} \text{~~if there is an attack}
  \end{cases}
\end{equation}

\paragraph*{\textbf{State modifications for action $u_i$}}
when choosing action $u_i$ at state $s$, we compute the budget impact and set changes for state $s', \forall j \in \textbf{N}$. 
\begin{equation}
  s \longrightarrow s' =\begin{cases}
    b_f^{s'} = b_f^s - c_f^i\\
    b_c^{s'} = b_c^s + c_c^ i\\
    Mo^{s'} = Mo^s \backslash\{i\}\\
    At^{s'} = At^s \cup \{j\}\text{~~if there is an attack}
  \end{cases}
\end{equation}
% \FC{$ Mo(s) \backslash \{n\}$ means removing n from Mo(s)}

\paragraph*{\textbf{State modifications for action $d$}}
when choosing action $d$ at state $s$, we compute the budget impact and set changes for state $s', \forall j \in \textbf{N}$. 
\begin{equation}
  s \longrightarrow s' =\begin{cases}
    b_f^{s'} = b_f^s - c_d\\
    At^{s'} = At^s \cup \{j\}\text{~~if there is an attack}
  \end{cases}
\end{equation}


% We note $\alpha$ the probability of an attack occurring, $q(j)$ the probability of node $j$ being the target of the attack.
% We have defined $q(j)$ with Algorithm~\ref{algo:target}.
We define the transition probability with $\alpha$ and $q(j)$ presented in Section~\ref{sec:target_proba}.
% When in state $s$, the system can transition to N+1 different states, whether an attack had not happened or, which one of the N nodes has been attacked.
% We define $S', s' \in S'$ the set of states to which the state $s$ can transition to.
% We define $s'_{j}$ as the state where node $j$ has been attacked and $s'_0$ as the state where no attack happened.

% Therefore, $s$ will transition with probability $1-\alpha$ when there is no attack or will transition with probability $q(j)$ when attacking node j:

% \begin{itemize}
%     \item $\forall a \in A, P(s,s',a)=1-\alpha$ 
%     \\when there is no attack
    
%     \item $\forall j \in \textbf{N}, \forall a \in A$ , $P(s,s',a)= q(j)$ 
%     \\when attacking node $j$.
% \end{itemize}
\begin{equation}
    P(s,s',a) = \begin{cases}
        1-\alpha \text{ if there is no attack}\\
        q(j)\text{ if node $j$ is attacked}
    \end{cases}
\end{equation}
\paragraph{Rewards}

The value of the reward for transitioning takes into accounts three criteria: the intrinsic value of the nodes, the overall progress of the attacker, and the probability of detecting an attack. The probability of an attack occurring is already accounted for in the transitions.
% The reward represents is impacted whether there was an attack, and if it has been detected.
% In addition to that, an attack only targets one node.
% In the previous sections, we have made assumptions on the path that will be used to attack a node.
% \FC{See suggestions}
If no attack was launched, the reward is based on the value of all the nodes in the infrastructure. 
If an attack was launched, there are two cases: whether the attacker has reached his ultimate goal or not.
If he has, we deduct the value of all the compromised nodes. If he has not, we only deduce the value of the attacked node.
% \GB{please use equation environments and refrain from using inline equations.}
%  note $(1-p)^{|L_j \cap Mo|}$ the probability of zero nodes detecting the attack on node j. 
\\We note $\Pi(j)=1 - (1-p)^{|L_j \cap Mo|}$ as the probability of at least one node detecting the attack on node $j$.
\\
Therefore, we define the following reward functions: %, with $s' \in S'$ as in Section~\ref{sec:actionset} :
\\
\begin{equation}
  R(s,a) =\begin{cases}
    \sum\limits_{i\in \textbf{N}} V_i \text{, if no attack}\\
    \sum\limits_{i\in \textbf{N}}^n V_i - \sum\limits_{k \in At^s} \overline{\Pi(k)}V_k \text{, if finalized attack }\\
    \sum\limits_{i\in \textbf{N}} V_i - \overline{\Pi(j)}V_j \text{, if partial attack}\\
  \end{cases}
\end{equation}





\subsubsection{Use Case}
\label{sec:numericalres}
In this section, we instantiate our MDP and perform several tests with varying input parameters.
We outline specific behaviors shown in the MDP optimal policy.
We have generated the MDP using the topology depicted in Fig.~\ref{fig:usecase}.
We have considered two scenarios: a) the attacker is located at node 6 only, b) the attacker is located at nodes 3 and 6.

We make the simulation computationally tractable by setting all financial and computational costs equal for all nodes, \ie $\forall i,j\in \textbf{N},~c_f^i=c_f^j=c_c^i=c_c^j=c_d=10$. For eased reading, we note the cost $c_a$.
Simply put, with $c_a=10$ and $b_c=40$ there will be a maximum of 4 nodes monitoring the infrastructure.
We set the discount factor of the MDP to 0.9 since the consequences of an attack are well-defined and can be precisely evaluated with risk assessment techniques.
We summarize the numerical values of the parameters in Table~\ref{tab:parameters}.


\input{figures/nodeimp-single.tex}
\input{figures/nodeimp-multiple.tex}

\paragraph{Numerical results}
We have run the MDP using different budgets and detection probabilities.
Nodes are migrated in the following order: 1, 2 and 3.
The ordering of the nodes impacts the result sets of Algorithm~\ref{algo:target}, thus which nodes may be attacked at each transition.
We have extracted the monitoring set of each absorbing state, and evaluated the overall reward of each monitoring set.
We define the reward of a monitoring set as the weighted mean of the reward of each corresponding absorbing state.
The weighted mean uses the stationary distribution of the Markov Chain corresponding to the optimal policy.
The results are shown in Fig.~\ref{fig:nodeimp_single} for scenario a) and in Fig.~\ref{fig:nodeimp_multiple} for scenario b).

Based on the paths taken by attacks and the ordering of the migration, we can categorize the nodes into three categories: source, intermediate and border nodes.
The scenario a) sets the source to node 6, nodes 4 and 5 as intermediate nodes and finally nodes 1,2 and 3 as border nodes.
The scenario b) sets the source to nodes 3 and 6, nodes 1,4 as intermediate and nodes 2 and 5 as border nodes.
The first observation is that node 6 is globally the most rewarding node in both cases.
This is explained as it is the source of most of the attacks, and the exfiltrated data is redirected there.
In both scenarios the importance of nodes is separated according to the our categorization. 
This implies that the more the nodes will be on the path of attacks the more they get rewarded.
This is observation is reinforced in scenario b) where nodes 4 and 5 are close to attack sources while nodes 1 and 2 are further away.

In both scenarios, the detection rate does not have a significant impact on the ranking of the nodes, compared to each other.
The main trend in scenario a) is from $p=0.6$  node 4 does not overcome node 6 in reward. All other nodes remain closely grouped, and no intermediate or border nodes is standing out.
In scenario b) the higher the detection rate the higher the reward of node 3, the secondary source of attacks.
The steadiness in the evolution of each node shows that the performance of the detection is not a major factor in determining which nodes are best suited for the monitoring. We formulate some hypotheses in Section~\ref{sec:discussion}.
% For the single attack source, the importance of nodes compared to each other remains approximately constant, which implies that globally the performance in detecting attacks does not impact much where the resources should be deployed.
% We will show that specific exceptions may arise in Section~\ref{sec:apriori}.
% For multiple sources we observe that the detection rate has a bigger impact on which node is prioritized.
% When the detection rate is low, the system tends to prefer intermediate nodes, and even node 3, the source of some attacks does not rank better (at $p=0.5$).

Detailed examination of the optimal policy for each budget also shows that the action $m_j$ is never used to redeploy resource elsewhere in the infrastructure.
Instead of unmonitoring nodes, the MDP chooses the action $d$  to preserve the global detection probability.
$m_j$ actions are only chosen when the unmonitored node does not detect the next attack, thus having the same impact as action $d$. These corner cases only represent a small percentage of the global solution where very few attacks occurred.


\input{figures/RA-parameters.tex}

\paragraph{A priori deployment}
\label{sec:apriori}
When solving a problem using an MDP, the solution is a dynamic proposition to choose actions as the system evolves.
However, from a technical aspect, the defender needs to have the nodes already monitoring the infrastructure before starting the migration process.
It becomes necessary to translate the dynamic answer of the MDP into a static \textit{a priori} deployment.
After determining the individual importance of each node, we propose to determine the optimal set of monitoring nodes.

The main difference is that each node was evaluated based on all the possible budget combinations, whereas what is defined here is a particular answer for a specific budget.
For each budget, the maximum reward is $\frac{b_f}{c_a} \sum\limits_{i \in \textbf{N}}V_i $ which corresponds to the corner case where the attacker never launched an attack, and we can evaluate the efficiency of the   monitoring nodes thanks to the associated reward.
Even if a particular monitoring set achieves close to the maximum reward, it is also because the set is tailored to a subset of all possible attacks.
We propose to determine the optimal monitoring state for each budget by weighting the reward they achieve with their occupation of the total solution space.

We note $S_{\text{abs}}$ the set of absorbing states, $S^{\text{Mo}}_{\text{abs}}$ the set of absorbing states with a common  monitoring set $Mo$, $\rho(Mo)$ the percentage of presence of set $Mo$ in the solution space and $R(Mo)$ the reward of monitoring set $Mo$.
% \begin{equation}
%     R(Mo) = \sum\limits_{s \in S_{Mo}^{abs}}\rho(Mo^s)R(Mo^s)
% \end{equation}
Then we propose to choose the optimal monitoring set $Mo^*$ with:
\begin{equation}
    Mo^* = \argmax\limits_{\text{Mo} \in S_{\text{abs}}} \left \{\sum\limits_{s \in S^{\text{Mo}}_{\text{abs}}}\rho(Mo^s)R(Mo^s) \right \}
\end{equation}

We present the results for scenario a) in Table~\ref{tab:optiset}.
We observe that nodes 4 and 6 are always chosen in the monitoring, which corresponds to Fig.~\ref{fig:nodeimp_single}.
Node 1 also often appears as a good candidate for a fourth node if it is not already chosen third.
With $p=0.7$ we observe that third and fourth nodes do not coincidate between (30,30) and (40,40) budgets.
This suggests that the combining two nodes increases their individual performance.
(Node 1 is surrounded by node 2 and 3 in the topology).
% This is explained because other budgets impact the individual importance of each node.

\input{figures/RA-optiset.tex}

\subsubsection{Discussion}
\label{sec:discussion}
In this section, we propose to discuss some of the limitations and findings of our approach.
The main limitation of the model is the size of the numerical use case. Since the MDP set of states and transitions are generated recursively, the bigger the topology and the budgets the bigger the computation time.
Because of the use of the $m_j$ action as an equivalent to action $d$, we assume that the relocation of monitoring resources would happen in bigger use cases, but those cannot be generated due to combinatory explosion.

In Section~\ref{sec:apriori} we proposed a method to determine for each budget what was the optimal monitoring set.
This method supposes that the ordering of the monitoring deployment does not impact the rewards obtained.
While this is not true when considering individually each path from the starting state to an absorbing state, the aggregation of the results toward monitoring sets reduces the impact of this assumption.
In addition to that, we have defined an attacker model in which the target decision is not based on which security measures are already deployed on the infrastructure, thus making the target decision space independent from the monitoring.
% Obviously, each monitoring decision will have a different impact for the defender depending on which node is being attacked for the next transition, but this does not affect the attacker.

We have observed the importance of the detection probability in our use case and concluded that it was not an impacting parameter in determining the optimal monitoring set.
However, topologies where important nodes could be reached from several paths and where there could be multiple data exfiltration paths could lead to choosing intermediate nodes over border nodes.